{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Numerical Embeddings for Tabular Deep Learning\n",
    "\n",
    "**Paper:** [On Embeddings for Numerical Features in Tabular Deep Learning](https://arxiv.org/abs/2203.05556)\n",
    "\n",
    "**Authors:** Yury Gorishniy, Ivan Rubachev, Artem Babenko (Yandex Research)\n",
    "\n",
    "**Venue:** NeurIPS 2022\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "The paper shows that **transforming scalar numerical features into high-dimensional embeddings** before mixing them in the main backbone (MLP, Transformer, etc.) significantly improves performance.\n",
    "\n",
    "Two main approaches:\n",
    "1. **Piecewise Linear Encoding (PLE):** Uses learnable bin boundaries\n",
    "2. **Periodic Embeddings:** Uses sin/cos functions with learnable frequencies\n",
    "\n",
    "The magic: Simple MLPs with these embeddings can match complex Transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import MLPPLR, PeriodicEmbeddings, PiecewiseLinearEncoding, compute_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Do We Need Numerical Embeddings?\n",
    "\n",
    "Consider a simple problem: predicting `y = sin(2πx)` where `x ∈ [0, 1]`.\n",
    "\n",
    "A standard MLP struggles with this because:\n",
    "- ReLU activations are piecewise linear\n",
    "- The network needs many layers to approximate smooth functions\n",
    "- Numerical features often have irregular distributions\n",
    "\n",
    "**Solution:** Transform `x` into a higher-dimensional representation that makes the target function easier to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the problem\n",
    "x = torch.linspace(0, 1, 100).unsqueeze(-1)\n",
    "y = torch.sin(2 * np.pi * x)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x.numpy(), y.numpy(), 'b-', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y = sin(2πx)')\n",
    "plt.title('Target Function: Hard to Approximate with ReLU MLPs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Piecewise Linear Encoding (PLE)\n",
    "\n",
    "PLE encodes a scalar `x` into a vector where each component represents \"how much\" the value falls into each bin.\n",
    "\n",
    "For bins `[b₀, b₁], [b₁, b₂], ...`:\n",
    "- If `x ≤ bᵢ`: encoding[i] = 0\n",
    "- If `x ≥ bᵢ₊₁`: encoding[i] = 1  \n",
    "- Otherwise: encoding[i] = (x - bᵢ) / (bᵢ₊₁ - bᵢ)\n",
    "\n",
    "This creates a **sparse, interpretable** representation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with 1 feature\n",
    "X_train = torch.randn(1000, 1)\n",
    "\n",
    "# Compute bins using quantiles\n",
    "bins = compute_bins(X_train, n_bins=8)\n",
    "print(f\"Bin boundaries for feature 0: {bins[0].numpy()}\")\n",
    "\n",
    "# Create PLE\n",
    "ple = PiecewiseLinearEncoding(bins)\n",
    "\n",
    "# Encode some test values\n",
    "test_values = torch.tensor([[bins[0][2].item()],   # Middle of bin 2\n",
    "                            [bins[0][4].item()],   # Middle of bin 4\n",
    "                            [bins[0][6].item()]])  # Middle of bin 6\n",
    "\n",
    "encodings = ple(test_values)\n",
    "print(f\"\\nEncoding shape: {encodings.shape}\")\n",
    "print(f\"\\nEncodings (sparse pattern visible):\")\n",
    "print(encodings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PLE encoding\n",
    "x_range = torch.linspace(X_train.min() - 0.5, X_train.max() + 0.5, 200).unsqueeze(-1)\n",
    "encodings = ple(x_range).numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot each bin's activation\n",
    "for i in range(encodings.shape[1]):\n",
    "    plt.plot(x_range.numpy(), encodings[:, i], label=f'Bin {i}')\n",
    "\n",
    "# Mark bin boundaries\n",
    "for b in bins[0].numpy():\n",
    "    plt.axvline(x=b, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Input value x')\n",
    "plt.ylabel('Encoding value')\n",
    "plt.title('Piecewise Linear Encoding: Each bin activates in its range')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Periodic Embeddings\n",
    "\n",
    "Periodic embeddings use sin/cos functions with **learnable frequencies**:\n",
    "\n",
    "```\n",
    "embed(x) = ReLU(Linear([sin(2π·f₁·x), cos(2π·f₁·x), sin(2π·f₂·x), ..., cos(2π·fₖ·x)]))\n",
    "```\n",
    "\n",
    "Where `f₁, f₂, ..., fₖ` are learnable frequency parameters.\n",
    "\n",
    "**Why it works:**\n",
    "- Similar to Fourier features in [Neural Tangent Kernels](https://arxiv.org/abs/2006.10739)\n",
    "- Network can learn which frequencies are important for the task\n",
    "- Captures periodic patterns naturally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create periodic embeddings\n",
    "periodic = PeriodicEmbeddings(\n",
    "    n_features=1,\n",
    "    d_embedding=8,\n",
    "    n_frequencies=4,\n",
    "    frequency_init_scale=0.1,  # Small init for stability\n",
    "    lite=False,\n",
    ")\n",
    "\n",
    "# Check learned frequencies\n",
    "print(f\"Initialized frequencies: {periodic.frequencies.data.squeeze().numpy()}\")\n",
    "print(f\"Output shape for single input: {periodic(torch.randn(1, 1)).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize periodic encoding before training\n",
    "x_range = torch.linspace(-2, 2, 200).unsqueeze(-1)\n",
    "with torch.no_grad():\n",
    "    embeddings = periodic(x_range)  # Shape: (200, 1, 8)\n",
    "    embeddings = embeddings.squeeze(1).numpy()  # Shape: (200, 8)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(embeddings.shape[1]):\n",
    "    plt.plot(x_range.numpy(), embeddings[:, i], label=f'Dim {i}')\n",
    "\n",
    "plt.xlabel('Input value x')\n",
    "plt.ylabel('Embedding value')\n",
    "plt.title('Periodic Embeddings: Each dimension captures different patterns')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLPPLR: MLP with Numerical Embeddings\n",
    "\n",
    "The full model architecture:\n",
    "\n",
    "```\n",
    "Input (batch, n_features)\n",
    "    ↓\n",
    "Embedding Layer (periodic or PLE)\n",
    "    ↓\n",
    "(batch, n_features, d_embedding)\n",
    "    ↓\n",
    "Flatten\n",
    "    ↓\n",
    "(batch, n_features × d_embedding)\n",
    "    ↓\n",
    "MLP Backbone (Linear → BN → ReLU → Dropout) × n_blocks\n",
    "    ↓\n",
    "Output (batch, d_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLPPLR with periodic embeddings\n",
    "model = MLPPLR(\n",
    "    d_in=10,\n",
    "    d_out=1,\n",
    "    d_embedding=24,\n",
    "    embedding_type=\"periodic\",\n",
    "    n_blocks=3,\n",
    "    d_block=128,\n",
    "    dropout=0.1,\n",
    "    n_frequencies=48,\n",
    "    frequency_init_scale=0.01,\n",
    ")\n",
    "\n",
    "print(f\"Total parameters: {model.count_parameters():,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(32, 10)\n",
    "out = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLPPLR with PLE (need to compute bins first)\n",
    "X_train = torch.randn(1000, 10)  # Simulated training data\n",
    "\n",
    "bins = compute_bins(X_train, n_bins=32)\n",
    "print(f\"Number of features: {len(bins)}\")\n",
    "print(f\"Bins per feature: {[len(b) - 1 for b in bins]}\")\n",
    "\n",
    "model_ple = MLPPLR(\n",
    "    d_in=10,\n",
    "    d_out=1,\n",
    "    d_embedding=16,\n",
    "    embedding_type=\"ple\",\n",
    "    bins=bins,\n",
    "    n_blocks=3,\n",
    "    d_block=128,\n",
    ")\n",
    "\n",
    "print(f\"\\nPLE model parameters: {model_ple.count_parameters():,}\")\n",
    "out = model_ple(torch.randn(32, 10))\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HFT/MFT Trading Applications\n",
    "\n",
    "Numerical embeddings are highly relevant for financial time series:\n",
    "\n",
    "### Why Periodic Embeddings Help:\n",
    "- **Intraday patterns:** Market behavior differs at open/close\n",
    "- **Round number effects:** Support/resistance at \\$100, \\$50 levels\n",
    "- **Cyclical indicators:** RSI, oscillators have periodic properties\n",
    "\n",
    "### Why PLE Helps:\n",
    "- **Price regimes:** Different behavior in different price ranges\n",
    "- **Volume bins:** High/low volume environments\n",
    "- **Volatility regimes:** Calm vs. turbulent markets\n",
    "\n",
    "### Computational Efficiency:\n",
    "- Both methods add minimal overhead\n",
    "- Suitable for real-time inference (unlike retrieval-based methods)\n",
    "- GPU-friendly operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Financial features\n",
    "feature_names = [\n",
    "    'returns_1m', 'returns_5m', 'returns_15m',  # Returns at different horizons\n",
    "    'volume_ratio',  # Volume vs average\n",
    "    'rsi_14',        # RSI (periodic by nature!)\n",
    "    'vwap_deviation',  # Distance from VWAP\n",
    "    'bid_ask_spread',  # Liquidity measure\n",
    "    'order_imbalance', # Order flow\n",
    "    'volatility_5m',   # Recent volatility\n",
    "    'time_of_day',     # Normalized time (periodic!)\n",
    "]\n",
    "\n",
    "# Create model tailored for HFT\n",
    "hft_model = MLPPLR(\n",
    "    d_in=len(feature_names),\n",
    "    d_out=1,  # Predict next return\n",
    "    d_embedding=24,\n",
    "    embedding_type=\"periodic\",  # Good for RSI, time_of_day\n",
    "    n_blocks=2,  # Shallow for speed\n",
    "    d_block=64,  # Small for speed\n",
    "    dropout=0.05,  # Light regularization\n",
    "    lite=True,  # Parameter efficient\n",
    ")\n",
    "\n",
    "print(f\"HFT model parameters: {hft_model.count_parameters():,}\")\n",
    "\n",
    "# Analyze learned frequencies\n",
    "print(f\"\\nFrequency stats: {hft_model.get_frequency_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmark Results\n",
    "\n",
    "From our benchmarks, MLPPLR achieves:\n",
    "\n",
    "| Dataset | MLPPLR | MLP | Best Overall |\n",
    "|---------|--------|-----|-------------|\n",
    "| friedman | 1.67 | 1.23 | TabR (1.12) |\n",
    "| nonlinear | **0.75** | 0.86 | **MLPPLR (0.75)** |\n",
    "| high_dim | **1.17** | 1.37 | **MLPPLR (1.17)** |\n",
    "| temporal | 1.22 | 1.24 | Temporal (0.61) |\n",
    "| mixed | 1.97 | 1.98 | TabM (1.97) |\n",
    "\n",
    "**Key insights:**\n",
    "- MLPPLR excels on **nonlinear** and **high-dimensional** data\n",
    "- Simple MLP baseline is hard to beat on basic regression\n",
    "- Embeddings shine when features have complex relationships with targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tips\n",
    "\n",
    "### Periodic Embeddings:\n",
    "- `frequency_init_scale`: **Critical!** Start small (0.01), tune up to 1.0\n",
    "- `n_frequencies`: 32-64 usually sufficient\n",
    "- `lite=True`: Use for efficiency, minimal performance loss\n",
    "\n",
    "### PLE:\n",
    "- `n_bins`: 32-64 for most tasks\n",
    "- Use tree-based bins when you have `y` for supervision\n",
    "- `activation=False` (version B) recommended\n",
    "\n",
    "### General:\n",
    "- `d_embedding`: 16-32 for MLP, larger for Transformers\n",
    "- Standard MLP hyperparameters (layers, width, dropout) still matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training example\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Generate synthetic data\n",
    "X = torch.randn(1000, 10)\n",
    "y = (X[:, 0] * X[:, 1] + torch.sin(X[:, 2] * 3)).unsqueeze(-1)  # Nonlinear target\n",
    "\n",
    "# Create model\n",
    "model = MLPPLR(d_in=10, d_out=1, embedding_type=\"periodic\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**On Embeddings for Numerical Features** (NeurIPS 2022) shows that:\n",
    "\n",
    "1. ✅ Simple scalar→vector transformations dramatically improve tabular DL\n",
    "2. ✅ Piecewise Linear Encoding (PLE) creates sparse, interpretable features\n",
    "3. ✅ Periodic embeddings capture complex patterns via learnable frequencies\n",
    "4. ✅ MLPs with embeddings can match Transformers at fraction of the cost\n",
    "5. ✅ Low overhead makes it practical for real-time trading systems\n",
    "\n",
    "**When to use:**\n",
    "- Features with irregular distributions\n",
    "- Complex, nonlinear feature-target relationships\n",
    "- Need speed (prefer over retrieval-based methods like TabR)\n",
    "- Financial data with periodic patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
