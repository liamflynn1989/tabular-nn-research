{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: iLTM - Integrated Large Tabular Model\n",
    "\n",
    "This tutorial explains the key ideas from the **iLTM** paper and demonstrates how to use our simplified implementation.\n",
    "\n",
    "**Paper:** [arXiv:2511.15941](https://arxiv.org/abs/2511.15941)  \n",
    "**Authors:** David Bonet, Marçal Comajoan Cara, Alvaro Calafell, Daniel Mas Montserrat, Alexander G. Ioannidis  \n",
    "**Venue:** arXiv 2025 (Stanford & UC Santa Cruz)\n",
    "\n",
    "## Overview\n",
    "\n",
    "iLTM is a **tabular foundation model** that integrates multiple approaches:\n",
    "\n",
    "1. **Tree-Derived Embeddings**: Uses GBDT leaf indices as features\n",
    "2. **Dimensionality-Agnostic Representations**: Random features + PCA for consistent embedding sizes\n",
    "3. **Meta-Trained Hypernetwork**: Generates MLP weights from training data\n",
    "4. **Retrieval-Augmented Predictions**: Soft k-NN blended with MLP output\n",
    "\n",
    "The key insight is that **tree-based and neural methods are complementary**:\n",
    "- GBDTs excel at capturing discrete feature interactions and are robust to uninformative features\n",
    "- MLPs learn smooth functions and benefit from gradient-based optimization\n",
    "- Retrieval provides local adaptivity and implicit ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from models import iLTM, create_iltm\n",
    "from models.iltm import TreeEmbedding, RandomFeatureProjection\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding GBDT Leaf Embeddings\n",
    "\n",
    "The first key idea in iLTM is to use **GBDT leaf indices as embeddings**.\n",
    "\n",
    "When a GBDT makes a prediction, each sample falls into exactly one leaf in each tree. By one-hot encoding these leaf indices, we get a sparse binary representation that captures:\n",
    "\n",
    "- **Non-linear feature interactions** discovered by the tree\n",
    "- **Robust feature selection** (uninformative features rarely appear in splits)\n",
    "- **Discrete patterns** (regime-like behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with non-linear interactions\n",
    "X, y = make_friedman1(n_samples=1000, n_features=10, noise=0.5, random_state=42)\n",
    "\n",
    "# Fit a GBDT and extract leaf embeddings\n",
    "tree_emb = TreeEmbedding(n_estimators=20, max_depth=4, task='regression')\n",
    "tree_emb.fit(X, y)\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = tree_emb.transform(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {tree_emb.embedding_dim} (sum of leaves across all trees)\")\n",
    "print(f\"Sparsity: {(embeddings == 0).mean():.2%} zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sparse embeddings for a few samples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Show embedding matrix for first 20 samples\n",
    "ax = axes[0]\n",
    "im = ax.imshow(embeddings[:20], aspect='auto', cmap='Blues')\n",
    "ax.set_xlabel('Leaf index (across all trees)')\n",
    "ax.set_ylabel('Sample')\n",
    "ax.set_title('GBDT Leaf Embeddings (One-Hot Encoded)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Show the number of leaves per tree\n",
    "ax = axes[1]\n",
    "ax.bar(range(len(tree_emb.n_leaves_per_tree)), tree_emb.n_leaves_per_tree)\n",
    "ax.set_xlabel('Tree index')\n",
    "ax.set_ylabel('Number of leaves')\n",
    "ax.set_title('Leaves per Tree in GBDT')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dimensionality-Agnostic Representation\n",
    "\n",
    "Different datasets have different numbers of features. To build a foundation model, we need a **fixed-size representation** regardless of input dimension.\n",
    "\n",
    "iLTM achieves this with:\n",
    "1. **Random Feature Expansion**: Project to high dimension using random matrix (approximates arc-cosine kernel)\n",
    "2. **PCA Reduction**: Reduce back to fixed dimension (512 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate dimensionality-agnostic projection\n",
    "proj = RandomFeatureProjection(d_out=64, n_random_features=1024)\n",
    "\n",
    "# Convert embeddings to tensor\n",
    "X_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "# Fit the projection\n",
    "proj.fit(X_tensor)\n",
    "\n",
    "# Transform to fixed dimension\n",
    "fixed_emb = proj(X_tensor)\n",
    "\n",
    "print(f\"Input shape: {X_tensor.shape}\")\n",
    "print(f\"Output shape: {fixed_emb.shape} (always d_out={proj.d_out} regardless of input dim)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the projection\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Original sparse embeddings\n",
    "ax = axes[0]\n",
    "ax.hist(embeddings.flatten(), bins=3, edgecolor='black')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('GBDT Embeddings (Sparse Binary)')\n",
    "\n",
    "# Projected dense embeddings\n",
    "ax = axes[1]\n",
    "ax.hist(fixed_emb.detach().numpy().flatten(), bins=50, edgecolor='black')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Projected Embeddings (Dense, Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Soft Retrieval Module\n",
    "\n",
    "The retrieval component finds **similar training examples** for each query and aggregates their labels.\n",
    "\n",
    "Unlike hard k-NN:\n",
    "- Uses **cosine similarity** in the learned embedding space\n",
    "- Applies **softmax** over similarities for smooth weighting\n",
    "- Supports **temperature scaling** to control sharpness\n",
    "- **Blends** with MLP predictions via α parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.iltm import SoftRetrievalModule\n",
    "\n",
    "# Create retrieval module\n",
    "retrieval = SoftRetrievalModule(\n",
    "    d_embedding=64,\n",
    "    temperature=1.0,\n",
    "    k_neighbors=10\n",
    ")\n",
    "\n",
    "# Use projected embeddings as our representation\n",
    "candidate_embeddings = fixed_emb  # (1000, 64)\n",
    "candidate_labels = torch.tensor(y, dtype=torch.float32)  # (1000,)\n",
    "\n",
    "# Query with a few test points\n",
    "query_embeddings = fixed_emb[:5]  # (5, 64)\n",
    "\n",
    "# Get retrieval predictions\n",
    "retrieval_output = retrieval(\n",
    "    query=query_embeddings,\n",
    "    candidates=candidate_embeddings,\n",
    "    candidate_labels=candidate_labels,\n",
    "    n_classes=None  # Regression mode\n",
    ")\n",
    "\n",
    "print(f\"Query shape: {query_embeddings.shape}\")\n",
    "print(f\"Retrieval output shape: {retrieval_output.shape}\")\n",
    "print(f\"\\nRetrieval predictions vs actual:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Sample {i}: predicted={retrieval_output[i].item():.3f}, actual={y[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Full iLTM Model\n",
    "\n",
    "Now let's see the complete iLTM model in action. The model:\n",
    "\n",
    "1. **Setup phase**: Fits GBDT and random projection on training data\n",
    "2. **Forward pass**: Computes embeddings → conditioning → MLP → retrieval blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print(f\"Training set: {X_train_t.shape}\")\n",
    "print(f\"Test set: {X_test_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iLTM model\n",
    "model = iLTM(\n",
    "    d_in=10,\n",
    "    d_out=1,\n",
    "    d_main=128,\n",
    "    n_blocks=2,\n",
    "    use_tree_embedding=True,\n",
    "    n_estimators=50,\n",
    "    retrieval_alpha=0.3,  # 30% retrieval, 70% MLP\n",
    "    k_neighbors=20,\n",
    ")\n",
    "\n",
    "# Setup the model (fits GBDT and projection)\n",
    "model.setup(X_train_t, y_train_t)\n",
    "print(\"Model setup complete!\")\n",
    "\n",
    "# Set candidates for retrieval\n",
    "model.set_candidates(X_train_t, y_train_t)\n",
    "print(f\"Set {len(y_train_t)} candidates for retrieval\")\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (without training the MLP)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_t)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = ((y_pred.flatten() - y_test_t) ** 2).mean().item()\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Test RMSE (without training): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the model briefly to see improvement\n",
    "import torch.optim as optim\n",
    "\n",
    "model.train()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_t)\n",
    "    loss = criterion(y_pred.flatten(), y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate after training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_t)\n",
    "\n",
    "mse = ((y_pred.flatten() - y_test_t) ** 2).mean().item()\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Test RMSE (after training): {rmse:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('iLTM Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Interpretability via Nearest Neighbors\n",
    "\n",
    "One advantage of retrieval-augmented models is **interpretability**. We can see which training examples influenced each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nearest neighbors for test samples\n",
    "model.eval()\n",
    "indices, similarities, neighbor_labels = model.get_nearest_neighbors(X_test_t[:5], k=5)\n",
    "\n",
    "print(\"Nearest neighbors for first 5 test samples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\nTest sample {i} (actual y = {y_test[i]:.3f})\")\n",
    "    print(f\"  Nearest neighbors (from training set):\")\n",
    "    for j in range(5):\n",
    "        idx = indices[i, j].item()\n",
    "        sim = similarities[i, j].item()\n",
    "        label = neighbor_labels[i, j].item()\n",
    "        print(f\"    Neighbor {j+1}: idx={idx:4d}, similarity={sim:.3f}, y={label:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Effect of Retrieval Alpha\n",
    "\n",
    "The `retrieval_alpha` parameter controls the blend between MLP and retrieval:\n",
    "- α = 0: Pure MLP (no retrieval)\n",
    "- α = 1: Pure retrieval (no MLP)\n",
    "- α = 0.3: 30% retrieval, 70% MLP (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values\n",
    "alphas = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model.retrieval_alpha = alpha\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_t)\n",
    "    rmse = np.sqrt(((y_pred.flatten() - y_test_t) ** 2).mean().item())\n",
    "    results.append(rmse)\n",
    "    print(f\"α = {alpha:.1f}: Test RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(alphas, results, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Retrieval Alpha (α)')\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.title('Effect of Retrieval Weight on Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Ideas from iLTM\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Tree Embeddings**: GBDT leaf indices capture discrete patterns and feature interactions\n",
    "2. **Dimensionality-Agnostic**: Random features + PCA create fixed-size representations\n",
    "3. **Soft Retrieval**: k-NN with temperature-scaled softmax enables smooth blending\n",
    "4. **Complementary Methods**: Trees + MLPs + Retrieval work better together\n",
    "\n",
    "### When to Use iLTM\n",
    "\n",
    "✅ **Good for:**\n",
    "- Datasets where GBDTs typically excel (structured/tabular)\n",
    "- Finding similar historical patterns (e.g., market regimes)\n",
    "- When interpretability matters (can inspect neighbors)\n",
    "- Varying input dimensions across datasets\n",
    "\n",
    "⚠️ **Limitations of this implementation:**\n",
    "- No pretrained hypernetwork (the key innovation of the paper)\n",
    "- Requires setup phase for each new dataset\n",
    "- Tree fitting adds overhead\n",
    "\n",
    "For the full pretrained model, see: https://github.com/AI-sandbox/iLTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
