{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabR: Retrieval-Augmented Tabular Deep Learning\n",
    "\n",
    "This tutorial explains TabR, a model that combines deep learning with k-NN-style retrieval. For each prediction, TabR retrieves similar training examples and uses attention to aggregate their information.\n",
    "\n",
    "**Paper:** [TabR: Tabular Deep Learning Meets Nearest Neighbors (ICLR 2024)](https://arxiv.org/abs/2307.14338)\n",
    "\n",
    "## Key Ideas\n",
    "\n",
    "1. **Retrieval-Augmented Prediction**: Instead of relying solely on learned weights, TabR retrieves similar examples from the training set to inform predictions.\n",
    "\n",
    "2. **Soft Attention Retrieval**: Unlike hard k-NN, TabR uses soft attention over candidates, making it end-to-end differentiable.\n",
    "\n",
    "3. **Label Integration**: Retrieved neighbors contribute both their features AND labels to the prediction.\n",
    "\n",
    "## Why TabR for Trading?\n",
    "\n",
    "- **Regime Detection**: Finds similar historical market patterns\n",
    "- **Explainability**: \"This prediction is based on these similar historical examples\"\n",
    "- **Noise Robustness**: Averaging over similar examples provides implicit ensembling\n",
    "- **Non-stationarity**: Can adapt by finding recent similar patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from models import TabR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview\n",
    "\n",
    "TabR has three main components:\n",
    "\n",
    "```\n",
    "Input (x) → Embeddings → Query\n",
    "                           ↓\n",
    "Candidates ────────────→ Retrieval (Attention) → Context\n",
    "                           ↓\n",
    "              [Query + Context] → MLP → Prediction\n",
    "```\n",
    "\n",
    "Let's create a model and examine its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TabR model\n",
    "model = TabR(\n",
    "    d_in=10,           # 10 input features\n",
    "    d_out=1,           # 1 output (regression)\n",
    "    d_embedding=24,    # Embedding dimension per feature\n",
    "    d_block=128,       # MLP hidden dimension\n",
    "    n_blocks=2,        # Number of MLP blocks\n",
    "    n_heads=4,         # Attention heads in retrieval\n",
    "    k_neighbors=64,    # Number of neighbors to retrieve\n",
    "    max_candidates=1000,  # Max candidates to store\n",
    ")\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Retrieval Works\n",
    "\n",
    "The retrieval module computes attention over all stored candidates:\n",
    "\n",
    "1. **Embed query and candidates** into a shared space\n",
    "2. **Compute attention scores** using dot product\n",
    "3. **Select top-k neighbors** for efficiency\n",
    "4. **Aggregate with softmax attention**\n",
    "\n",
    "The key insight is that this is differentiable, so gradients flow back to improve embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "torch.manual_seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Create data with a non-linear pattern\n",
    "X_train = torch.randn(n_samples, 10)\n",
    "y_train = (\n",
    "    2 * torch.sin(X_train[:, 0] * 2) +\n",
    "    X_train[:, 1] ** 2 +\n",
    "    X_train[:, 2] * X_train[:, 3] +\n",
    "    0.5 * torch.randn(n_samples)  # noise\n",
    ").unsqueeze(1)\n",
    "\n",
    "print(f\"Training data: X={X_train.shape}, y={y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training with Candidate Accumulation\n",
    "\n",
    "During training, TabR accumulates candidates from training batches. This is done automatically when you pass `y_for_candidates`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "losses = []\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Random batches\n",
    "    perm = torch.randperm(n_samples)\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        x_batch = X_train[idx]\n",
    "        y_batch = y_train[idx]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass y_for_candidates to accumulate candidates during training\n",
    "        pred = model(x_batch, y_for_candidates=y_batch)\n",
    "        \n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    losses.append(epoch_loss / (n_samples // batch_size))\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {losses[-1]:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpretability: Finding Similar Examples\n",
    "\n",
    "One of TabR's key advantages is interpretability. We can see which training examples influenced a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Create a test sample\n",
    "x_test = torch.randn(5, 10)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    pred = model(x_test)\n",
    "    \n",
    "print(\"Predictions:\", pred.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nearest neighbors for interpretability\n",
    "k = 5\n",
    "indices, distances, neighbor_labels = model.get_nearest_neighbors(x_test, k=k)\n",
    "\n",
    "print(f\"\\nFor each test sample, showing {k} nearest neighbors:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    print(f\"\\nTest sample {i}: Predicted = {pred[i].item():.3f}\")\n",
    "    print(f\"  Neighbors (labels): {neighbor_labels[i].numpy()}\")\n",
    "    print(f\"  Mean neighbor label: {neighbor_labels[i].mean():.3f}\")\n",
    "    print(f\"  Distances: {distances[i].numpy()[:3]}...\")  # Show first 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Embedding Space\n",
    "\n",
    "Let's visualize how TabR embeds examples and finds neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Embed training data\n",
    "    train_emb = model.embeddings(X_train).view(n_samples, -1).numpy()\n",
    "    test_emb = model.embeddings(x_test).view(len(x_test), -1).numpy()\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "all_emb = np.vstack([train_emb, test_emb])\n",
    "all_2d = pca.fit_transform(all_emb)\n",
    "\n",
    "train_2d = all_2d[:n_samples]\n",
    "test_2d = all_2d[n_samples:]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot training points colored by label\n",
    "scatter = ax.scatter(\n",
    "    train_2d[:, 0], train_2d[:, 1],\n",
    "    c=y_train.squeeze().numpy(),\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=30,\n",
    "    label='Training'\n",
    ")\n",
    "plt.colorbar(scatter, label='Target value')\n",
    "\n",
    "# Plot test points\n",
    "ax.scatter(\n",
    "    test_2d[:, 0], test_2d[:, 1],\n",
    "    c='red',\n",
    "    marker='*',\n",
    "    s=200,\n",
    "    edgecolors='black',\n",
    "    label='Test queries'\n",
    ")\n",
    "\n",
    "# Draw lines to neighbors for first test point\n",
    "test_idx = 0\n",
    "for neighbor_idx in indices[test_idx]:\n",
    "    ax.plot(\n",
    "        [test_2d[test_idx, 0], train_2d[neighbor_idx, 0]],\n",
    "        [test_2d[test_idx, 1], train_2d[neighbor_idx, 1]],\n",
    "        'r--', alpha=0.3\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('TabR Embedding Space (2D PCA projection)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trading Application: Market Regime Detection\n",
    "\n",
    "TabR is particularly useful for identifying similar historical market conditions. Here's a conceptual example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate market features\n",
    "np.random.seed(42)\n",
    "n_days = 1000\n",
    "\n",
    "# Features: [volatility, momentum, volume, spread, ...]\n",
    "market_features = np.column_stack([\n",
    "    np.random.exponential(0.02, n_days),  # volatility\n",
    "    np.random.randn(n_days) * 0.01,       # momentum\n",
    "    np.random.lognormal(0, 0.5, n_days),  # volume\n",
    "    np.random.exponential(0.001, n_days), # spread\n",
    "    np.random.randn(n_days, 6) * 0.1      # other features\n",
    "])\n",
    "\n",
    "# Simulate returns (depends on features with noise)\n",
    "returns = (\n",
    "    0.1 * market_features[:, 1] -         # momentum effect\n",
    "    0.5 * market_features[:, 0] +         # volatility drag\n",
    "    0.02 * np.random.randn(n_days)        # noise\n",
    ")\n",
    "\n",
    "# Create TabR model for market prediction\n",
    "market_model = TabR(\n",
    "    d_in=10,\n",
    "    d_out=1,\n",
    "    d_embedding=16,\n",
    "    k_neighbors=20,\n",
    "    max_candidates=500,\n",
    ")\n",
    "\n",
    "X_market = torch.tensor(market_features, dtype=torch.float32)\n",
    "y_market = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"Market data: {n_days} days, {market_features.shape[1]} features\")\n",
    "print(f\"Return range: [{returns.min():.4f}, {returns.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on historical data\n",
    "market_model.train()\n",
    "optimizer = torch.optim.Adam(market_model.parameters(), lr=0.001)\n",
    "\n",
    "train_size = 800\n",
    "X_hist = X_market[:train_size]\n",
    "y_hist = y_market[:train_size]\n",
    "\n",
    "for epoch in range(30):\n",
    "    optimizer.zero_grad()\n",
    "    pred = market_model(X_hist, y_for_candidates=y_hist)\n",
    "    loss = nn.MSELoss()(pred, y_hist)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Training complete. Final loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current market conditions\n",
    "market_model.eval()\n",
    "current_day = X_market[850:851]  # Pick a test day\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = market_model(current_day)\n",
    "    indices, distances, similar_returns = market_model.get_nearest_neighbors(current_day, k=10)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MARKET REGIME ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPredicted return: {prediction.item():.4f}\")\n",
    "print(f\"Actual return: {y_market[850].item():.4f}\")\n",
    "print(f\"\\nSimilar historical days (returns): {similar_returns.squeeze().numpy()[:5]}\")\n",
    "print(f\"Average return in similar conditions: {similar_returns.mean():.4f}\")\n",
    "print(f\"Std of returns in similar conditions: {similar_returns.std():.4f}\")\n",
    "\n",
    "# This information can be used for:\n",
    "# 1. Confidence estimation (low std = high confidence)\n",
    "# 2. Risk management (check similar days for drawdowns)\n",
    "# 3. Explainability (\"prediction based on these similar days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "1. **TabR combines the best of both worlds**: Deep learning representation + k-NN retrieval\n",
    "\n",
    "2. **Interpretable**: You can always explain predictions by showing similar training examples\n",
    "\n",
    "3. **Works best with moderate dimensions**: For very high-dimensional data (50+ features), consider dimensionality reduction first\n",
    "\n",
    "4. **Trading applications**:\n",
    "   - Market regime detection\n",
    "   - Confidence estimation from neighbor variance\n",
    "   - Risk management via historical analogs\n",
    "   - Regulatory-compliant explainability\n",
    "\n",
    "5. **Limitations**:\n",
    "   - Memory scales with number of candidates\n",
    "   - Retrieval slower for high-dimensional embeddings\n",
    "   - Curse of dimensionality affects k-NN component"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
