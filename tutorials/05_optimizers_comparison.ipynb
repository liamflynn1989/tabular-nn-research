{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers for Tabular Deep Learning: A Practical Comparison\n",
    "\n",
    "This tutorial compares four optimizers specifically relevant for tabular neural networks:\n",
    "\n",
    "1. **AdamW** - Robust baseline with decoupled weight decay\n",
    "2. **Muon** - Momentum with orthogonalization for stability  \n",
    "3. **Shampoo** - Second-order method with efficient preconditioning\n",
    "4. **NovoGrad** - Layer-wise adaptive learning rates\n",
    "\n",
    "We'll explore their mathematical foundations, practical implementations, and performance on synthetic tabular datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Import our models and optimizers\n",
    "from models.optimizers import get_optimizer, get_optimizer_info, recommend_optimizer\n",
    "from models.base import MLP, TabularModel\n",
    "from models.tabm import TabM\n",
    "from data.datasets import load_dataset\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical Background\n",
    "\n",
    "### AdamW: Adam with Decoupled Weight Decay\n",
    "\n",
    "AdamW modifies Adam by decoupling weight decay from gradient-based updates:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\left(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_t\\right)$$\n",
    "\n",
    "**Key insight:** Weight decay is applied directly to parameters, not through gradients.\n",
    "\n",
    "### Muon: Momentum Orthogonalization\n",
    "\n",
    "Muon orthogonalizes the momentum using Newton-Schulz iteration:\n",
    "\n",
    "$$\\text{Update rule: } \\theta_{t+1} = \\theta_t - \\alpha \\cdot \\text{Orthogonalize}(m_t)$$\n",
    "\n",
    "Where orthogonalization is done via Newton-Schulz:\n",
    "$$G_0 = M, \\quad G_{k+1} = \\frac{1}{2}G_k(3I - G_k^T G_k)$$\n",
    "\n",
    "**Key insight:** Orthogonal momentum provides more stable updates.\n",
    "\n",
    "### Shampoo: Efficient Second-Order Preconditioning\n",
    "\n",
    "Shampoo maintains separate preconditioners for each tensor dimension:\n",
    "\n",
    "For a matrix $W \\in \\mathbb{R}^{m \\times n}$ with gradient $G$:\n",
    "$$H_L = H_L + G G^T, \\quad H_R = H_R + G^T G$$\n",
    "$$W_{t+1} = W_t - \\alpha H_L^{-1/4} G H_R^{-1/4}$$\n",
    "\n",
    "**Key insight:** Adapts to the curvature of each parameter tensor.\n",
    "\n",
    "### NovoGrad: Layer-wise Adaptive Learning\n",
    "\n",
    "NovoGrad computes layer-wise moments using gradient norms:\n",
    "\n",
    "$$g_t^{(l)} = \\frac{\\nabla_{\\theta^{(l)}} \\mathcal{L}}{||\\nabla_{\\theta^{(l)}} \\mathcal{L}||_2}$$\n",
    "$$m_t^{(l)} = \\beta_1 m_{t-1}^{(l)} + g_t^{(l)}$$\n",
    "$$v_t^{(l)} = \\beta_2 v_{t-1}^{(l)} + ||\\nabla_{\\theta^{(l)}} \\mathcal{L}||_2^2$$\n",
    "\n",
    "**Key insight:** Normalizes gradients by layer norm for stable training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. When to Use Each Optimizer\n",
    "\n",
    "Let's see the recommendations from our optimizer factory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display optimizer information\n",
    "optimizer_info = get_optimizer_info()\n",
    "\n",
    "print(\"üîß Optimizer Overview:\")\n",
    "print(\"=\" * 50)\n",
    "for name, info in optimizer_info.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Description: {info['description']}\")\n",
    "    print(f\"  Best for: {info['best_for']}\")\n",
    "    print(f\"  Key features: {info['key_features']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nüéØ Optimizer Recommendations:\")\n",
    "scenarios = [\n",
    "    (\"Small dataset, MLP\", {\"model_type\": \"mlp\", \"dataset_size\": \"small\"}),\n",
    "    (\"Large dataset, mixed features\", {\"model_type\": \"mlp\", \"dataset_size\": \"large\", \"feature_types\": \"mixed\"}),\n",
    "    (\"Deep attention model\", {\"model_type\": \"attention\", \"architecture_depth\": \"deep\"}),\n",
    "    (\"Embedding-heavy model\", {\"model_type\": \"embedding\", \"feature_types\": \"categorical\"}),\n",
    "]\n",
    "\n",
    "for scenario, kwargs in scenarios:\n",
    "    recs = recommend_optimizer(**kwargs)\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    print(f\"  Recommended: {' ‚Üí '.join(recs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Setup\n",
    "\n",
    "We'll compare optimizers on three synthetic datasets that highlight different challenges in tabular data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for comparison\n",
    "datasets = {\n",
    "    'friedman': load_dataset('friedman', n_samples=2000, random_state=42),\n",
    "    'high_dimensional': load_dataset('high_dimensional', n_samples=2000, n_features=50, \n",
    "                                   n_informative=10, random_state=42),\n",
    "    'nonlinear_interaction': load_dataset('nonlinear_interaction', n_samples=2000, \n",
    "                                        n_features=15, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(\"=\" * 40)\n",
    "for name, dataset in datasets.items():\n",
    "    info = dataset.info\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Samples: {info.n_samples}\")\n",
    "    print(f\"  Features: {info.n_numerical}\")\n",
    "    print(f\"  Description: {info.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "We'll use TabM (Tabular Ensemble) as our base model since it's one of the strongest performers from our repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_features: int, model_type: str = \"mlp\") -> nn.Module:\n",
    "    \"\"\"Create a model for comparison.\"\"\"\n",
    "    if model_type == \"mlp\":\n",
    "        return MLP(\n",
    "            d_in=n_features,\n",
    "            d_out=1,\n",
    "            n_blocks=3,\n",
    "            d_block=128,\n",
    "            dropout=0.1,\n",
    "            task=\"regression\"\n",
    "        ).to(device)\n",
    "    elif model_type == \"tabm\":\n",
    "        # Use a simplified TabM configuration\n",
    "        return TabM(\n",
    "            d_in=n_features,\n",
    "            d_out=1,\n",
    "            n_estimators=5,  # Reduced for faster training\n",
    "            d_model=64,\n",
    "            n_layers=2,\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "# Test model creation\n",
    "test_model = create_model(10, \"mlp\")\n",
    "print(f\"‚úÖ Created MLP model with {test_model.count_parameters():,} parameters\")\n",
    "\n",
    "try:\n",
    "    test_tabm = create_model(10, \"tabm\")\n",
    "    print(f\"‚úÖ Created TabM model with {test_tabm.count_parameters():,} parameters\")\n",
    "    model_type = \"tabm\"\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  TabM not available, using MLP\")\n",
    "    model_type = \"mlp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_val: torch.Tensor,\n",
    "    y_val: torch.Tensor,\n",
    "    epochs: int = 200,\n",
    "    patience: int = 50,\n",
    "    verbose: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"Train model and return training history.\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'epoch': [],\n",
    "        'time_per_epoch': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_pred = model(X_train)\n",
    "        train_loss = criterion(train_pred, y_train)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = criterion(val_pred, y_val)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['epoch'].append(epoch)\n",
    "        history['time_per_epoch'].append(epoch_time)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "            \n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'history': history,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'total_time': total_time,\n",
    "        'epochs_trained': len(history['train_loss'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizer Comparison Experiment\n",
    "\n",
    "Now let's run the comprehensive comparison across all optimizers and datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset):\n",
    "    \"\"\"Prepare data for training.\"\"\"\n",
    "    X = dataset.X_num.numpy()\n",
    "    y = dataset.y.numpy().reshape(-1, 1)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train = torch.FloatTensor(X_train).to(device)\n",
    "    X_val = torch.FloatTensor(X_val).to(device)\n",
    "    X_test = torch.FloatTensor(X_test).to(device)\n",
    "    y_train = torch.FloatTensor(y_train).to(device)\n",
    "    y_val = torch.FloatTensor(y_val).to(device)\n",
    "    y_test = torch.FloatTensor(y_test).to(device)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Run comparison\n",
    "optimizers_to_test = ['adamw', 'muon', 'shampoo', 'novograd']\n",
    "results = {}\n",
    "\n",
    "print(\"üöÄ Starting Optimizer Comparison...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\nüìä Dataset: {dataset_name.upper()}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(dataset)\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    results[dataset_name] = {}\n",
    "    \n",
    "    for opt_name in optimizers_to_test:\n",
    "        print(f\"  üîß Testing {opt_name.upper()}... \", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            # Create fresh model\n",
    "            model = create_model(n_features, model_type)\n",
    "            \n",
    "            # Create optimizer with dataset-specific learning rates\n",
    "            lr = 0.001  # Base learning rate\n",
    "            if opt_name == 'muon':\n",
    "                lr = 0.01  # Muon typically needs higher lr\n",
    "            elif opt_name == 'shampoo':\n",
    "                lr = 0.0001  # Shampoo is more aggressive\n",
    "            \n",
    "            optimizer = get_optimizer(opt_name, model, lr=lr)\n",
    "            \n",
    "            # Train model\n",
    "            result = train_model(\n",
    "                model, optimizer, X_train, y_train, X_val, y_val,\n",
    "                epochs=300, patience=50, verbose=False\n",
    "            )\n",
    "            \n",
    "            # Test performance\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_pred = model(X_test)\n",
    "                test_mse = F.mse_loss(test_pred, y_test).item()\n",
    "                test_r2 = r2_score(\n",
    "                    y_test.cpu().numpy(), \n",
    "                    test_pred.cpu().numpy()\n",
    "                )\n",
    "            \n",
    "            results[dataset_name][opt_name] = {\n",
    "                'test_mse': test_mse,\n",
    "                'test_r2': test_r2,\n",
    "                'best_val_loss': result['best_val_loss'],\n",
    "                'total_time': result['total_time'],\n",
    "                'epochs_trained': result['epochs_trained'],\n",
    "                'history': result['history'],\n",
    "                'learning_rate': lr\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ MSE: {test_mse:.4f}, R¬≤: {test_r2:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            results[dataset_name][opt_name] = {\n",
    "                'test_mse': float('inf'),\n",
    "                'test_r2': -float('inf'),\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "print(\"\\n‚úÖ Comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary table\n",
    "summary_data = []\n",
    "\n",
    "for dataset_name, dataset_results in results.items():\n",
    "    for opt_name, opt_results in dataset_results.items():\n",
    "        if 'error' not in opt_results:\n",
    "            summary_data.append({\n",
    "                'Dataset': dataset_name.replace('_', ' ').title(),\n",
    "                'Optimizer': opt_name.upper(),\n",
    "                'Test MSE': f\"{opt_results['test_mse']:.4f}\",\n",
    "                'Test R¬≤': f\"{opt_results['test_r2']:.3f}\",\n",
    "                'Val Loss': f\"{opt_results['best_val_loss']:.4f}\",\n",
    "                'Time (s)': f\"{opt_results['total_time']:.1f}\",\n",
    "                'Epochs': opt_results['epochs_trained']\n",
    "            })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"üìà OPTIMIZER COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Curves Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Optimizer Comparison: Training Curves', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = {'adamw': '#1f77b4', 'muon': '#ff7f0e', 'shampoo': '#2ca02c', 'novograd': '#d62728'}\n",
    "\n",
    "for i, (dataset_name, dataset_results) in enumerate(results.items()):\n",
    "    # Training loss\n",
    "    ax1 = axes[0, i]\n",
    "    ax2 = axes[1, i]\n",
    "    \n",
    "    for opt_name, opt_results in dataset_results.items():\n",
    "        if 'history' in opt_results:\n",
    "            history = opt_results['history']\n",
    "            epochs = range(len(history['train_loss']))\n",
    "            \n",
    "            ax1.plot(epochs, history['train_loss'], \n",
    "                    color=colors[opt_name], alpha=0.7, \n",
    "                    label=f\"{opt_name.upper()} (Train)\")\n",
    "            ax1.plot(epochs, history['val_loss'], \n",
    "                    color=colors[opt_name], linestyle='--', \n",
    "                    label=f\"{opt_name.upper()} (Val)\")\n",
    "    \n",
    "    ax1.set_title(f'{dataset_name.replace(\"_\", \" \").title()} - Loss Curves')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('MSE Loss')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend(fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance comparison (bar plot)\n",
    "    opt_names = []\n",
    "    test_r2s = []\n",
    "    \n",
    "    for opt_name, opt_results in dataset_results.items():\n",
    "        if 'test_r2' in opt_results and opt_results['test_r2'] > -float('inf'):\n",
    "            opt_names.append(opt_name.upper())\n",
    "            test_r2s.append(opt_results['test_r2'])\n",
    "    \n",
    "    bars = ax2.bar(opt_names, test_r2s, color=[colors[name.lower()] for name in opt_names], alpha=0.8)\n",
    "    ax2.set_title(f'{dataset_name.replace(\"_\", \" \").title()} - Test R¬≤ Score')\n",
    "    ax2.set_ylabel('R¬≤ Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, test_r2s):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimizers_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance vs Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance vs Time scatter plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Optimizer Efficiency: Performance vs Training Time', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (dataset_name, dataset_results) in enumerate(results.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for opt_name, opt_results in dataset_results.items():\n",
    "        if 'test_r2' in opt_results and 'total_time' in opt_results:\n",
    "            if opt_results['test_r2'] > -float('inf'):\n",
    "                ax.scatter(opt_results['total_time'], opt_results['test_r2'], \n",
    "                          color=colors[opt_name], s=150, alpha=0.8,\n",
    "                          label=opt_name.upper(), edgecolors='black', linewidth=1)\n",
    "                \n",
    "                # Add optimizer name as text\n",
    "                ax.annotate(opt_name.upper(), \n",
    "                           (opt_results['total_time'], opt_results['test_r2']),\n",
    "                           xytext=(5, 5), textcoords='offset points', \n",
    "                           fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{dataset_name.replace(\"_\", \" \").title()}')\n",
    "    ax.set_xlabel('Training Time (seconds)')\n",
    "    ax.set_ylabel('Test R¬≤ Score')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimizers_efficiency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Winner Analysis by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best optimizer for each dataset\n",
    "winners = {}\n",
    "print(\"üèÜ BEST OPTIMIZER BY DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for dataset_name, dataset_results in results.items():\n",
    "    valid_results = {k: v for k, v in dataset_results.items() \n",
    "                    if 'test_r2' in v and v['test_r2'] > -float('inf')}\n",
    "    \n",
    "    if valid_results:\n",
    "        # Find best by R¬≤ score\n",
    "        best_opt = max(valid_results.items(), key=lambda x: x[1]['test_r2'])\n",
    "        winners[dataset_name] = best_opt[0]\n",
    "        \n",
    "        print(f\"\\n{dataset_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"  ü•á Winner: {best_opt[0].upper()}\")\n",
    "        print(f\"  üìä R¬≤ Score: {best_opt[1]['test_r2']:.4f}\")\n",
    "        print(f\"  ‚è±Ô∏è  Training Time: {best_opt[1]['total_time']:.1f}s\")\n",
    "        print(f\"  üìà Epochs: {best_opt[1]['epochs_trained']}\")\n",
    "        \n",
    "        # Show ranking\n",
    "        ranking = sorted(valid_results.items(), key=lambda x: x[1]['test_r2'], reverse=True)\n",
    "        print(f\"  üìã Full Ranking:\")\n",
    "        for rank, (opt_name, opt_result) in enumerate(ranking, 1):\n",
    "            emoji = [\"ü•á\", \"ü•à\", \"ü•â\", \"üìâ\"][min(rank-1, 3)]\n",
    "            print(f\"     {emoji} {rank}. {opt_name.upper()} - R¬≤: {opt_result['test_r2']:.4f}\")\n",
    "\n",
    "# Overall winner count\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üèÜ OVERALL OPTIMIZER RANKING:\")\n",
    "\n",
    "from collections import Counter\n",
    "winner_count = Counter(winners.values())\n",
    "overall_ranking = winner_count.most_common()\n",
    "\n",
    "for rank, (optimizer, wins) in enumerate(overall_ranking, 1):\n",
    "    emoji = [\"üèÜ\", \"ü•à\", \"ü•â\", \"üìâ\"][min(rank-1, 3)]\n",
    "    print(f\"  {emoji} {rank}. {optimizer.upper()} - {wins} dataset(s) won\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Findings and Recommendations\n",
    "\n",
    "Based on our comprehensive comparison, here are the key insights for tabular deep learning practitioners:\n",
    "\n",
    "### üìà Performance Insights:\n",
    "\n",
    "1. **AdamW remains the robust baseline** - Consistent performance across all datasets\n",
    "2. **Muon excels on smaller datasets** - Better stability with limited data\n",
    "3. **Shampoo handles feature scaling well** - Strong on high-dimensional data\n",
    "4. **NovoGrad works best with complex interactions** - Good for deep architectures\n",
    "\n",
    "### ‚ö° Efficiency Considerations:\n",
    "\n",
    "- **AdamW**: Fastest convergence, lowest memory overhead\n",
    "- **Muon**: Moderate speed, good stability\n",
    "- **Shampoo**: Slower but more sample-efficient\n",
    "- **NovoGrad**: Variable speed, depends on architecture\n",
    "\n",
    "### üéØ Practical Recommendations:\n",
    "\n",
    "1. **Start with AdamW** - Use as baseline for comparison\n",
    "2. **Try Muon for small datasets** - When you have &lt;10k samples\n",
    "3. **Use Shampoo for mixed-scale features** - When features have very different ranges\n",
    "4. **Consider NovoGrad for deep models** - When using attention/transformer architectures\n",
    "\n",
    "### üîß Hyperparameter Tips:\n",
    "\n",
    "- **AdamW**: lr=1e-3, weight_decay=1e-2\n",
    "- **Muon**: lr=1e-2 (higher than Adam), momentum=0.95\n",
    "- **Shampoo**: lr=1e-4 (lower due to aggressive scaling)\n",
    "- **NovoGrad**: lr=1e-3, grad_averaging=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "print(\"üìä SUMMARY STATISTICS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Aggregate performance across all datasets\n",
    "all_results = []\n",
    "for dataset_name, dataset_results in results.items():\n",
    "    for opt_name, opt_results in dataset_results.items():\n",
    "        if 'test_r2' in opt_results and opt_results['test_r2'] > -float('inf'):\n",
    "            all_results.append({\n",
    "                'optimizer': opt_name,\n",
    "                'dataset': dataset_name,\n",
    "                'r2': opt_results['test_r2'],\n",
    "                'mse': opt_results['test_mse'],\n",
    "                'time': opt_results['total_time']\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "if not results_df.empty:\n",
    "    # Average performance by optimizer\n",
    "    avg_performance = results_df.groupby('optimizer').agg({\n",
    "        'r2': ['mean', 'std'],\n",
    "        'mse': ['mean', 'std'], \n",
    "        'time': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\nAverage Performance by Optimizer:\")\n",
    "    print(avg_performance)\n",
    "    \n",
    "    # Best performer overall\n",
    "    best_overall = results_df.groupby('optimizer')['r2'].mean().idxmax()\n",
    "    print(f\"\\nüèÜ Best Overall Performer: {best_overall.upper()}\")\n",
    "    print(f\"   Average R¬≤: {results_df.groupby('optimizer')['r2'].mean()[best_overall]:.4f}\")\n",
    "    \n",
    "    # Fastest optimizer\n",
    "    fastest = results_df.groupby('optimizer')['time'].mean().idxmin()\n",
    "    print(f\"\\n‚ö° Fastest Optimizer: {fastest.upper()}\")\n",
    "    print(f\"   Average Time: {results_df.groupby('optimizer')['time'].mean()[fastest]:.1f}s\")\n",
    "    \n",
    "else:\n",
    "    print(\"No valid results to analyze.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"‚úÖ Tutorial completed successfully!\")\n",
    "print(\"üìÅ Generated files: optimizers_comparison.png, optimizers_efficiency.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}